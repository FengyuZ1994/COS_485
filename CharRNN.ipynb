{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CharRNN.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1ezg4K2VBe2BqmMd43XGukMESExF3wgDM","timestamp":1524685388079}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"qml7Iv9sKwYG","colab_type":"text"},"cell_type":"markdown","source":["#COS 485 Character level RNN\n","The source is based on https://github.com/spro/char-rnn.pytorch"]},{"metadata":{"id":"BsvMU7Q7KwzP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# http://pytorch.org/\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eKc2CJilRWYR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip install -q tqdm\n","from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BsD1YE9g76O2","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"zOGCa5JDK17n","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import torch\n","import math\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import torchvision\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.autograd import Variable\n","import torchvision.transforms as transforms\n","from IPython import display\n","import time\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r4kame7P215c","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["use_cuda = False"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zK3wy8XmLM1d","colab_type":"text"},"cell_type":"markdown","source":["# Dataset\n","Download Shakespeare, preprocess and Display some examples"]},{"metadata":{"id":"52KWKp--LRVH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":153},"outputId":"12571caa-cae5-44f3-c5ca-dd68ea91042a","executionInfo":{"status":"ok","timestamp":1524857162023,"user_tz":240,"elapsed":698,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["import requests\n","import string\n","import random\n","\n","all_characters = string.printable\n","n_characters = len(all_characters)\n","\n","def DownloadFile(url):\n","    local_filename = url.split('/')[-1]\n","    r = requests.get(url)\n","    return r.text\n","\n","def char_tensor(string):\n","    tensor = torch.zeros(len(string)).long()\n","    for c in range(len(string)):\n","        try:\n","            tensor[c] = all_characters.index(string[c])\n","        except:\n","            continue\n","    return tensor  \n","\n","def random_training_set(chunk_len, batch_size, file):\n","    inp = torch.LongTensor(batch_size, chunk_len)\n","    target = torch.LongTensor(batch_size, chunk_len)\n","    for bi in range(batch_size):\n","        start_index = random.randint(0, len(file) - chunk_len)\n","        end_index = start_index + chunk_len + 1\n","        chunk = file[start_index:end_index]\n","        inp[bi] = char_tensor(chunk[:-1])\n","        target[bi] = char_tensor(chunk[1:])\n","    inp = Variable(inp)\n","    target = Variable(target)\n","    if use_cuda:\n","        inp = inp.cuda()\n","        target = target.cuda()\n","    return inp, target\n","\n","def time_since(since):\n","    s = time.time() - since\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","  \n","target_url = \"https://raw.githubusercontent.com/cos495/code/master/shakespeare.txt\"\n","data = DownloadFile(target_url)\n","#print(random_training_set(10, 8, data))\n","print(data[10:100])"],"execution_count":6,"outputs":[{"output_type":"stream","text":["zen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n"],"name":"stdout"}]},{"metadata":{"id":"Uw7E1yvHPbMD","colab_type":"text"},"cell_type":"markdown","source":["#Model\n","In this code we use Pytorch already implemented Recurrent Neural Network Cell computation with `nn.RNN` and `nn.LSTM`"]},{"metadata":{"id":"YM-RJy9gNcT3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# https://github.com/spro/char-rnn.pytorch\n","class CharRNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, model=\"rnn\", n_layers=1):\n","        super(CharRNN, self).__init__()\n","        self.model = model.lower()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","\n","        self.encoder = nn.Embedding(input_size, hidden_size)\n","        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n","        if model==\"lstm\":\n","          self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n","          \n","        self.decoder = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input, hidden):\n","        batch_size = input.size(0)\n","        encoded = self.encoder(input)\n","        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n","        output = self.decoder(output.view(batch_size, -1))\n","        return output, hidden\n","\n","    def init_hidden(self, batch_size):\n","        if self.model == \"lstm\":\n","            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n","                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n","        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QjolCWdP1fkS","colab_type":"text"},"cell_type":"markdown","source":["#Train"]},{"metadata":{"id":"KB9xsWiEQhkU","colab_type":"text"},"cell_type":"markdown","source":["###Iinitialize the model"]},{"metadata":{"id":"ZDV_0GRbLtXw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["hidden_size = 100\n","learning_rate = 0.01\n","cell = \"rnn\"\n","n_layers = 2\n","\n","decoder = CharRNN(\n","    n_characters,\n","    hidden_size,\n","    n_characters,\n","    model=cell,\n","    n_layers=n_layers,\n",")\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","if use_cuda:\n","    decoder.cuda()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GRdaN2-FRndV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["n_epochs = 2000\n","chunk_len = 200\n","print_every = 100\n","batch_size = 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5KPmkvGJPkNt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def train(inp, target):\n","    hidden = decoder.init_hidden(batch_size)\n","    if use_cuda:\n","        hidden = hidden.cuda()\n","    decoder.zero_grad()\n","    loss = 0\n","\n","    for c in range(chunk_len):\n","        output, hidden = decoder(inp[:,c], hidden)\n","        loss += criterion(output.view(batch_size, -1), target[:,c])\n","\n","    loss.backward()\n","    decoder_optimizer.step()\n","\n","    return loss.data[0] / chunk_len"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V8M-FSO-TTLF","colab_type":"text"},"cell_type":"markdown","source":["# Generate Text"]},{"metadata":{"id":"Gh4cIaRDS23s","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=False):\n","    hidden = decoder.init_hidden(1)\n","    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n","\n","    if cuda:\n","        hidden = hidden.cuda()\n","        prime_input = prime_input.cuda()\n","    predicted = prime_str\n","\n","    # Use priming string to \"build up\" hidden state\n","    for p in range(len(prime_str) - 1):\n","        _, hidden = decoder(prime_input[:,p], hidden)\n","        \n","    inp = prime_input[:,-1]\n","    \n","    for p in range(predict_len):\n","        output, hidden = decoder(inp, hidden)\n","        \n","        # Sample from the network as a multinomial distribution\n","        output_dist = output.data.view(-1).div(temperature).exp()\n","        top_i = torch.multinomial(output_dist, 1)[0]\n","\n","        # Add predicted character to string and use as next input\n","        predicted_char = all_characters[top_i]\n","        predicted += predicted_char\n","        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n","        if cuda:\n","            inp = inp.cuda()\n","\n","    return predicted"],"execution_count":0,"outputs":[]},{"metadata":{"id":"riXDDbny1-DF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":2312},"outputId":"a886e63d-61fd-4542-afc3-9658a1cf1086","executionInfo":{"status":"ok","timestamp":1524858971177,"user_tz":240,"elapsed":1353097,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["start = time.time()\n","all_losses = []\n","loss_avg = 0\n","\n","print(\"Training for %d epochs...\" % n_epochs)\n","for epoch in tqdm(range(1, n_epochs + 1)):\n","    loss = train(*random_training_set(chunk_len, batch_size, data))\n","    loss_avg += loss\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n","        print('loss: ', loss)\n","        print(generate(decoder, 'Wh', 100, cuda=use_cuda), '\\n')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/2000 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training for 2000 epochs...\n"],"name":"stdout"},{"output_type":"stream","text":["  5%|▌         | 100/2000 [01:08<21:44,  1.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["[1m 8s (100 5%) 1.7881]\n","loss:  1.7881326293945312\n","Which seine to hear not for the sink now, struch from to whick capparoughy the hering ipon you\n","I rest  \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 10%|█         | 200/2000 [02:16<20:30,  1.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["[2m 16s (200 10%) 1.6331]\n","loss:  1.633067626953125\n","Which to the more to her:\n","Thee for blows thearies and it fair the old to the back my do; than my sain  \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 15%|█▌        | 300/2000 [03:24<19:17,  1.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["[3m 24s (300 15%) 1.5988]\n","loss:  1.5987567138671874\n","Whyself shrity, with a mother complone.\n","\n","JULIET:\n","Ehan with my, she to a kneen to come, what is the sou \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 20%|██        | 400/2000 [04:31<18:04,  1.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["[4m 31s (400 20%) 1.5535]\n","loss:  1.5535232543945312\n","What conserricious look best the rather, with offection\n","Unnarage?\n","\n","First Servants to him severate to o \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 25%|██▌       | 500/2000 [05:38<16:55,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[5m 38s (500 25%) 1.5560]\n","loss:  1.5559506225585937\n","Whone desion out with honour and daughter! O, and I have hastings unclaim in this wit the rest be this \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 30%|███       | 600/2000 [06:45<15:45,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[6m 45s (600 30%) 1.5153]\n","loss:  1.5152717590332032\n","Why, that 'I hath the creparry, my lady, consent not bastes become to than all\n","And commons by my heade \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 35%|███▌      | 700/2000 [07:52<14:37,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[7m 52s (700 35%) 1.4771]\n","loss:  1.477056427001953\n","Why was most dieldon\n","To seen more to fools, then, somethmanblotted speak with honest and the need.\n","\n","LU \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 40%|████      | 800/2000 [08:59<13:29,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[8m 59s (800 40%) 1.4911]\n","loss:  1.4910984802246094\n","What is not my father,\n","The purpome, and shall stipit.\n","\n","MENENIUS:\n","I speak, and thou art thou sister thi \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 45%|████▌     | 900/2000 [10:07<12:22,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[10m 7s (900 45%) 1.5156]\n","loss:  1.5156369018554687\n","What is thy dires are well his dambles to do.\n","\n","GREMIO:\n","Beseive he first! and thou wast be stand thee,  \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 1000/2000 [11:15<11:15,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[11m 15s (1000 50%) 1.4844]\n","loss:  1.4844248962402344\n","Which shall she king him to me, I can I have dut's wrong to the title that art day, to have answer eno \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 55%|█████▌    | 1100/2000 [12:23<10:08,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[12m 23s (1100 55%) 1.4735]\n","loss:  1.4734890747070313\n","Which state upon far speakness'd pass!\n","\n","AUTOLYCUS:\n","What what wing to him;\n","And provoke the profits are  \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 60%|██████    | 1200/2000 [13:31<09:01,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[13m 31s (1200 60%) 1.5023]\n","loss:  1.5023362731933594\n","Who crown, I\n","Amen; were the morneretences him of him;\n","Was play incless them he cannot my lord my graci \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 65%|██████▌   | 1300/2000 [14:39<07:53,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[14m 39s (1300 65%) 1.4652]\n","loss:  1.4651730346679688\n","What, she hath some gentle own nack.\n","\n","FRIAR LAURENCE:\n","To levent.\n","If that profio,\n","For it it will not in \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 70%|███████   | 1400/2000 [15:46<06:45,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[15m 46s (1400 70%) 1.4395]\n","loss:  1.439539794921875\n","While and like more no hay issignot the sweet Lord:\n","Without another!\n","We till stay; and have hastings,  \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 75%|███████▌  | 1500/2000 [16:54<05:38,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[16m 54s (1500 75%) 1.4714]\n","loss:  1.4714414978027344\n","Wherefore I will I do comes the godst less but the surmeed as hasth\n","The fire.\n","\n","LUCIO:\n","Most father, the \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|████████  | 1600/2000 [18:01<04:30,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[18m 1s (1600 80%) 1.4457]\n","loss:  1.4456869506835937\n","Whose it, she shall be more on the good,\n","Than some sprear with sweet work.\n","\n","CLAUDIO:\n","Then, but was yel \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 85%|████████▌ | 1700/2000 [19:11<03:23,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[19m 11s (1700 85%) 1.4469]\n","loss:  1.4469297790527345\n","Whist you is thy lords.\n","\n","MAMILLIUS:\n","That we shall must be your crain in sure.;\n","Why then in prove, do s \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 90%|█████████ | 1800/2000 [20:18<02:15,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[20m 18s (1800 90%) 1.4564]\n","loss:  1.4564405822753905\n","While here let that house thy master'd how this love stricks, thunkel lose the poor proms,\n","The people  \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 95%|█████████▌| 1900/2000 [21:25<01:07,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[21m 25s (1900 95%) 1.4482]\n","loss:  1.4481936645507814\n","Whither yet.\n","\n","KING RICHARD III:\n","She in proved,\n","Which that no chilite of sorrow and riege me Murge shal \n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2000/2000 [22:32<00:00,  1.48it/s]"],"name":"stderr"},{"output_type":"stream","text":["[22m 32s (2000 100%) 1.4456]\n","loss:  1.4456103515625\n","What love and grave?\n","\n","DUKE VINCENTIO:\n","That here 'go to the boys you do;\n","Sab'st both on a presers: poor \n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"metadata":{"id":"7snNirbPzaDk","colab_type":"text"},"cell_type":"markdown","source":["\n","### Let's try sampling with high temperature:"]},{"metadata":{"id":"0oCPS6kzRK4w","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"6fa3585e-875c-4b64-d54b-6fb89cd7e400","executionInfo":{"status":"ok","timestamp":1524860483813,"user_tz":240,"elapsed":375,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["generate(decoder, prime_str=\"A\", temperature= 100, cuda=use_cuda)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"A9\\x0c,\\rh!a3nBO,cJ1U^#SA|rxu7Ho.-g2|(E.]B*y|8-'3lO-,{=CjXyd'/+@%CVzYuXC)nBrlt~;E`>+\\r=:me)+k>u[iHg-:Y5w;j\""]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"q1g9IOY-zmZ8","colab_type":"text"},"cell_type":"markdown","source":["### Let's try sampling with low temperature:"]},{"metadata":{"id":"77ijhzn9Tc1C","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"b5bd88a8-2030-4147-8102-635eaabe8ac3","executionInfo":{"status":"ok","timestamp":1524860494926,"user_tz":240,"elapsed":279,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["generate(decoder, prime_str=\"A\", temperature= 0.001, cuda=use_cuda)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Abaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaabaab'"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"LyFpPpgJzss4","colab_type":"text"},"cell_type":"markdown","source":["### Describe the difference\n","How do the samples qualitatively change? What does changing the temperature do to distribution of possible outputs?¶\n","\n","**High temperatures cause the model to take more chances and increase diversity of results, but at a cost of more mistakes. Low temperature will cause the model to make more likely, but also more conservative predictions.** \n"]},{"metadata":{"id":"eMhO23CE4ap9","colab_type":"text"},"cell_type":"markdown","source":["### Let's try sampling with reasonable temperature:"]},{"metadata":{"id":"Ub5LAaAh4Z_y","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"e8617fff-f50d-4f23-f7a6-7e6d6acc4fea","executionInfo":{"status":"ok","timestamp":1524862502590,"user_tz":240,"elapsed":275,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["generate(decoder, prime_str=\"A\", temperature= 0.25, cuda=use_cuda)"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'And shall be his father, and the state to the world, the proceed to the bring the lands the prove the'"]},"metadata":{"tags":[]},"execution_count":29}]},{"metadata":{"id":"wVmZ28NK43PU","colab_type":"text"},"cell_type":"markdown","source":["**When Temperature is around 0.25, the model generate a reasonable sentence based on the prime_str \"A\"**"]},{"metadata":{"id":"xND5BVvo1pxa","colab_type":"text"},"cell_type":"markdown","source":["### Example\n","Insert most meaningful sentence that the network generated, change `prime_str`"]},{"metadata":{"id":"nkfQIbosztV_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"e5c0f101-21ed-46be-cf86-3be48b3090a8","executionInfo":{"status":"ok","timestamp":1524862261815,"user_tz":240,"elapsed":297,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["generate(decoder, prime_str=\"why\", cuda=use_cuda)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'why, and to my life, sir, we thou hast more good,\\nOr shows for a good and king, and high to kill for hi'"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"sctA3BJD7Uvu","colab_type":"text"},"cell_type":"markdown","source":["### Repeat the sample experiments with nn.LSTM"]},{"metadata":{"id":"UxY0FAOd7Sbx","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["hidden_size = 100\n","learning_rate = 0.01\n","cell = \"lstm\"\n","n_layers = 2\n","\n","decoder = CharRNN(\n","    n_characters,\n","    hidden_size,\n","    n_characters,\n","    model=cell,\n","    n_layers=n_layers,\n",")\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","if use_cuda:\n","    decoder.cuda()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zBP48ZWK7mV7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["n_epochs = 2000\n","chunk_len = 200\n","print_every = 100\n","batch_size = 100"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z0Wz7xzT7nt7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def train(inp, target):\n","    hidden = decoder.init_hidden(batch_size)\n","    if use_cuda:\n","        hidden = hidden.cuda()\n","    decoder.zero_grad()\n","    loss = 0\n","\n","    for c in range(chunk_len):\n","        output, hidden = decoder(inp[:,c], hidden)\n","        loss += criterion(output.view(batch_size, -1), target[:,c])\n","\n","    loss.backward()\n","    decoder_optimizer.step()\n","\n","    return loss.data[0] / chunk_len"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9lbXl-vl7oT2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":2397},"outputId":"966db1fe-495f-415d-fc10-2d0b800170f8","executionInfo":{"status":"ok","timestamp":1524869527193,"user_tz":240,"elapsed":2869491,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["start = time.time()\n","all_losses = []\n","loss_avg = 0\n","\n","print(\"Training for %d epochs...\" % n_epochs)\n","for epoch in tqdm(range(1, n_epochs + 1)):\n","    loss = train(*random_training_set(chunk_len, batch_size, data))\n","    loss_avg += loss\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n","        print('loss: ', loss)\n","        print(generate(decoder, 'Wh', 100, cuda=use_cuda), '\\n')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/2000 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training for 2000 epochs...\n"],"name":"stdout"},{"output_type":"stream","text":["  5%|▌         | 100/2000 [02:29<47:12,  1.49s/it]"],"name":"stderr"},{"output_type":"stream","text":["[2m 28s (100 5%) 1.3378]\n","loss:  1.337754669189453\n","Which we hear me for worthing shall bears me to\n","epoice of his love, sir: a shin'd admitted of other ch \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 10%|█         | 200/2000 [04:55<44:18,  1.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["[4m 55s (200 10%) 1.3370]\n","loss:  1.3369523620605468\n","Which I shall my husband here would and weep the house,\n","That a brother Randasted of all\n","water shall le \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 15%|█▌        | 300/2000 [07:21<41:39,  1.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["[7m 21s (300 15%) 1.3156]\n","loss:  1.3156253051757814\n","Where I will says you are they a name to\n","most death of this head in this acted as the rest.\n","\n","COMINIUS: \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 20%|██        | 400/2000 [09:45<39:02,  1.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["[9m 45s (400 20%) 1.3071]\n","loss:  1.3071195983886719\n","Which her light no bewing in this part good again\n","That must by the throne's mission,\n","And leave us deat \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 25%|██▌       | 500/2000 [12:09<36:28,  1.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["[12m 9s (500 25%) 1.3012]\n","loss:  1.3011962890625\n","Which left with more of this depart\n",": Countetford at the day, and with stretched much fight.\n","\n","CORIOLAN \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 30%|███       | 600/2000 [14:32<33:55,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[14m 32s (600 30%) 1.3155]\n","loss:  1.3154891967773437\n","What\n","I am glesses to blow betimes, good Lucio,\n","Thy grave his mid it of a gentleman,\n","And leave note in  \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 35%|███▌      | 700/2000 [16:57<31:29,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[16m 57s (700 35%) 1.3299]\n","loss:  1.3298904418945312\n","Why heart,\n","Or how we will dared to the sun, for I\n","fare will all this is a place, and they then well ag \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 40%|████      | 800/2000 [19:22<29:04,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[19m 22s (800 40%) 1.3004]\n","loss:  1.3003900146484375\n","What are some from Coriolioring!\n","They go a word and Loverabe:\n","The sin that but there! Stand Jood see t \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 45%|████▌     | 900/2000 [21:47<26:37,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[21m 47s (900 45%) 1.3193]\n","loss:  1.3192735290527344\n","Where is my soul, I warrant your good sore!\n","\n","PETRUCHIO:\n","Villain!\n","\n","PERDITA:\n","O, then they canst thou has \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 50%|█████     | 1000/2000 [24:10<24:10,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[24m 10s (1000 50%) 1.3013]\n","loss:  1.3013052368164062\n","Where is the rest:\n","What thorth of his heart and to a deed; there\n","I came and sound to ruin you now, wit \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 55%|█████▌    | 1100/2000 [26:37<21:47,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[26m 37s (1100 55%) 1.2836]\n","loss:  1.2836212158203124\n","What to see slever him appears his life,\n","The account you and his power: he did myself,\n","We shall gueds  \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 60%|██████    | 1200/2000 [29:03<19:22,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[29m 3s (1200 60%) 1.3305]\n","loss:  1.3304866027832032\n","Wherefore, mercy commend of a follo,\n","And rash flood or this death.\n","\n","LEONTES:\n","Cousin, my goodly little, \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 65%|██████▌   | 1300/2000 [31:27<16:56,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[31m 27s (1300 65%) 1.3111]\n","loss:  1.3111160278320313\n","Whith each of the redfright begin,\n","And that thou art thou wilt be means\n","Is well-adanted on all prove y \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 70%|███████   | 1400/2000 [33:52<14:31,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[33m 52s (1400 70%) 1.2861]\n","loss:  1.286078643798828\n","Why, that you like a town being son.\n","\n","KING RICHARD III:\n","So more, this is my heart, my lord!\n","\n","GLOUCESTE \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 75%|███████▌  | 1500/2000 [36:17<12:05,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[36m 17s (1500 75%) 1.2830]\n","loss:  1.282981414794922\n","Where Margaret death is rantaguain,\n","To be not, and it is their cleaks of the vice of the war.\n","\n","PRINCE: \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 80%|████████  | 1600/2000 [38:41<09:40,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[38m 40s (1600 80%) 1.3163]\n","loss:  1.3162733459472655\n","Where spoke no morning with the reserves of\n","while, not, with commilumpner than the man: then.\n","\n","CAMILLO \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 85%|████████▌ | 1700/2000 [41:04<07:14,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[41m 4s (1700 85%) 1.2913]\n","loss:  1.291254425048828\n","Where all she could think by no Dercesse them obedient his street?\n","\n","HENRY BOLINGBROKE:\n","Find, that is a \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 90%|█████████ | 1800/2000 [43:28<04:49,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[43m 28s (1800 90%) 1.3074]\n","loss:  1.3074148559570313\n","Which who be gone with the time and\n","lices of good contracted as so both be breath.\n","\n","Second Murderer:\n","I \n","\n"],"name":"stdout"},{"output_type":"stream","text":[" 95%|█████████▌| 1900/2000 [45:52<02:24,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[45m 52s (1900 95%) 1.2940]\n","loss:  1.2940036010742189\n","What consent anger is thy house;\n","Shall I would not shall the prince to enjoy him,\n","Even no more short!  \n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 2000/2000 [48:17<00:00,  1.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["[48m 17s (2000 100%) 1.2852]\n","loss:  1.28523681640625\n","Where is yours; but the tincest!\n","\n","LARTIUS:\n","How hate my son:\n","Thou hast truth to the chance that would I \n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"metadata":{"id":"5vOTFTEfXTnU","colab_type":"text"},"cell_type":"markdown","source":["\n","### Let's try sampling with high temperature:"]},{"metadata":{"id":"FVv3ngrFW-km","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"923894cc-6ffb-4b5b-bd04-a91dc685664b","executionInfo":{"status":"ok","timestamp":1524870427463,"user_tz":240,"elapsed":347,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["generate(decoder, prime_str=\"A\", temperature= 100, cuda=use_cuda)"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'AmJ%xuz.\"7!4dZOcMlG#v+//63Rjre-LT/^\"zFMN4{@!3#YDi<\\nM!!h\\r?Cy~0X$lC7g>jiZ4(1(zCHwu\\x0cm*Z+Z/(f_^`>)OU**;m='"]},"metadata":{"tags":[]},"execution_count":36}]},{"metadata":{"id":"9C8KVh1vXU6_","colab_type":"text"},"cell_type":"markdown","source":["\n","### Let's try sampling with low temperature:"]},{"metadata":{"id":"uYbHTNCCXElV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"8e210ee7-c8ce-430c-bd39-3bedd48e1253","executionInfo":{"status":"ok","timestamp":1524870441175,"user_tz":240,"elapsed":245,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["generate(decoder, prime_str=\"A\", temperature= 0.001, cuda=use_cuda)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Aaabaaaaabaaaaabaaaaabaaaaabaaaaabaaaaabaaaaacaaaacaaaabaaaaabaaaaabaaaaabaaaaabaaaaabaaaaacaaaacaaaa'"]},"metadata":{"tags":[]},"execution_count":37}]},{"metadata":{"id":"WKIXPm_kXW1v","colab_type":"text"},"cell_type":"markdown","source":["\n","### Let's try sampling with reasonable temperature:"]},{"metadata":{"id":"ddO90WOLXEpb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":34},"outputId":"1a795d25-9b17-47f0-c2da-6ec5c2495f87","executionInfo":{"status":"ok","timestamp":1524870470826,"user_tz":240,"elapsed":270,"user":{"displayName":"Fengyu Zhang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"112442328420773017950"}}},"cell_type":"code","source":["generate(decoder, prime_str=\"A\", temperature= 0.25, cuda=use_cuda)"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'And then I have been a cause to the state\\nThe sea and the best of the state of his poor brother\\nAnd t'"]},"metadata":{"tags":[]},"execution_count":41}]},{"metadata":{"id":"w3JEoX0s7A1O","colab_type":"text"},"cell_type":"markdown","source":["### Cross Entropy per letter in bits and Perplexity results on prediction"]},{"metadata":{"id":"wQ6NCgia4H2d","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"NeQCs_BX7M7p","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"WvU9zelr7ivL","colab_type":"text"},"cell_type":"markdown","source":["#Cell Types"]},{"metadata":{"id":"6x4ITg_Fv7zY","colab_type":"text"},"cell_type":"markdown","source":["###Elman Cell Computation\n","An Elman RNN cell with tanh or ReLU non-linearity.\n","\n","$h' = \\tanh(w_{ih} x + b_{ih}  +  w_{hh} h + b_{hh})$"]},{"metadata":{"id":"WMLga3QYv4es","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class RNNCell(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n","        super(RNNCell, self).__init__()\n","        #Implement initializations\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        \n","        self.W_ih = Variable(nn.random_normal([self.input_size, self.hidden_size],\n","                                       mean=0, stddev=0.2))\n","        self.W_hh = Variable(nn.random_normal([self.hidden_size, self.hidden_size],\n","                                       mean=0, stddev=0.2))\n","        self.b_ih = Variable(nn.zeros([1, self.hidden_size]))\n","        self.b_hh = Variable(nn.zeros([1, self.hidden_size]))\n","        self.V = Variable(nn.random_normal([self.hidden_size, self.output_size],\n","                                       mean=0, stddev=0.2))\n","        self.b_hy = Variable(nn.zeros([1, self.output_size]))\n","        \n","        \n","    def forward(self, input, hidden):\n","      \n","        hidden = nn.sigmoid(torch.matmul(input, self.W_ih) + torch.matmul(hidden, self.W_hh) + self.b_ih + self.b_hh)\n","        output = torch.matmul(hidden, self.V) + self.b_hy\n","        \n","        #Implement forward pass\n","        return output, hidden"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2oQt7o-GtzT7","colab_type":"text"},"cell_type":"markdown","source":["### LSTM Cell Computation\n","Implement LSTM cell computation described by the following expression"]},{"metadata":{"id":"cN4pgOp-syDx","colab_type":"text"},"cell_type":"markdown","source":["$i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n","f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n","g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n","o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n","c_t = f_t c_{(t-1)} + i_t g_t \\\\\n","h_t = o_t \\tanh(c_t)$\n"]},{"metadata":{"id":"a-kRdJRYsKmM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class LSTMCell(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n","        super(LSTMCell, self).__init__()\n","        #Implement initializations\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        \n","        self.W_ii = Variable(torch.random_normal([self.hidden_size, self.hidden_size], mean=0, stddev=0.2))\n","        self.W_hi = Variable(torch.random_normal([self.input_size, self.hidden_size], mean=0, stddev=0.2))\n","        self.b_ii = Variable(torch.zeros([1, self.hidden_size]))\n","        self.b_hi = Variable(torch.zeros([1, self.hidden_size]))\n","        \n","        self.W_hf = Variable(torch.random_normal([self.hidden_size, self.hidden_size], mean=0, stddev=0.2))\n","        self.W_if = Variable(torch.random_normal([self.input_size, self.hidden_size], mean=0, stddev=0.2))\n","        self.b_if = Variable(torch.zeros([1, self.hidden_size]))\n","        self.b_hf = Variable(torch.zeros([1, self.hidden_size]))\n","        \n","        self.W_hg = Variable(torch.random_normal([self.hidden_size, self.hidden_size], mean=0, stddev=0.2))\n","        self.W_ig = Variable(torch.random_normal([self.input_size, self.hidden_size], mean=0, stddev=0.2))\n","        self.b_hg = Variable(torch.zeros([1, self.hidden_size]))\n","        self.b_ig = Variable(torch.zeros([1, self.hidden_size]))\n","        \n","        self.W_ho = Variable(torch.random_normal([self.hidden_size, self.hidden_size], mean=0, stddev=0.2))\n","        self.W_io = Variable(torch.random_normal([self.input_size, self.hidden_size], mean=0, stddev=0.2))\n","        self.b_ho = Variable(torch.zeros([1, self.hidden_size]))\n","        self.b_io = Variable(torch.zeros([1, self.hidden_size]))\n","        \n","        \n","    def forward(self, input, hidden, cell):\n","        #Implement forward pass\n","        i = nn.sigmoid(torch.matmul(input, self.W_ii) + tf.matmul(hidden, self.W_hi) + self.b_ii + self.b_hi)\n","        f = nn.sigmoid(torch.matmul(input, self.W_if) + tf.matmul(hidden, self.W_hf) + self.b_if + self.b_hf)\n","        g = nn.tanh(torch.matmul(input, self.W_ig) + tf.matmul(hidden, self.W_hg) + self.b_ig + self.b_hg)\n","        o = nn.sigmoid(torch.matmul(input, self.W_io) + tf.matmul(hidden, self.W_ho) + self.b_io + self.b_ho)\n","        c = torch.mul(f, cell) + torch.mul(i, g)\n","        h = torch.mul(o, c)\n","        \n","        output = o \n","        hidden = h\n","        cell = c\n","        \n","        return output, hidden, cell\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l0izKDP403t2","colab_type":"text"},"cell_type":"markdown","source":["### LSTM Gates\n","List all gates that LSTM uses and describe their role"]},{"metadata":{"id":"p4rEoPcD1CxE","colab_type":"text"},"cell_type":"markdown","source":["ft : the forget gate layer, It looks the last previous hidden ouptut h(t-1) and the input x, and outputs a number between 0 and 1. A 1\n","represents “completely keep this” while a 0 represents “completely get rid of this.”\n","\n","it and gt : the input gate layer, decides which values the network will update. The gate \"ft\" keep or forget whether the previous info will be kept or not. The gate \"it\" and \"gt\" will add to the next output cell. \n","\n","ot : the output gate layer, which controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit. \n","\n","\n"]},{"metadata":{"id":"VCPJRlYtzCXG","colab_type":"text"},"cell_type":"markdown","source":["### Explain how LSTM Cell is different than Simple RNN? (why is it better or worse?)"]},{"metadata":{"id":"WP86OZ3b1Eyy","colab_type":"text"},"cell_type":"markdown","source":["The difference in structure: Simple RNN has a single neroun like strucutre. But LSTM cell has much more complex structures in a repeating unit, including input gate, forget gate, output gate and memory cell. \n","\n","The difference in performance: It is difficult for a standard simple RNN to solve problems that require learning long-term temporal dependencies. LSTM cells include a 'memory cell' that can maintain information in memory for long periods of time. This architecture lets LSTM learn longer-term dependencies. Simple RNNs suffer from vanishing and exploding gradient problems. LSTMs deal with these problems by introducing new gates, such as input and forget gates, which allow for a better control over the gradient flow and enable better preservation of “long-range dependencies”.\n"]},{"metadata":{"id":"pe9TPf__7yaJ","colab_type":"text"},"cell_type":"markdown","source":["#Train CharRNN with your LSTM cell(Extra)"]},{"metadata":{"id":"x3onffCP7wxs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}